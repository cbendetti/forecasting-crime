---
title: "Forecasting Crime in Chicago"
author: "Camilla Bendetti and Alexander Foster"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  html_document:
    toc: true
    toc_float: true
bibliography: "Library.bib"
abstract: In this report, we will explore how historical crime data can be used to predict future crimes in the City of Chicago. A few cities in California have started to implement a predictive policing algorithm, called PredPol, that does just this. PredPol outputs "500x500 square foot" locations for officers to patrol in their "uncommitted time" [@Lam2019] and has been found to be "ten times as efficient as random patrolling" [@o2016weapons]. We use a marginal model to forecast future crimes and we visualize our findings using spatial data in order to explore predictive policing in Chicago.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warnings=FALSE)
library(sf)
library(stringr)
library(lubridate)
library(dplyr)
library(ggplot2)
library(maptools)
library(ape)
library(tidyr)
library(spdep)
require(gridExtra)
library(geeM)
library(spatstat)
```

## Introduction

At one point, in the city of Ferguson, Missouri, the statistics suggested "that 92% of black residents" had an "open arrest" warrant. [@basu2019wrongs] If you were a police officer and at the beginning of your shift you received this statistic, how would you approach your job for the rest of the day? In “The Wrongs of Racist Beliefs”, Rima Basu provides a thought experiment of two police officers that, at the beginning of their shift, receive this statistic. One of the officers, Stella, uses this statistical evidence to stop any black resident she sees in order to check if they have a warrant out for their arrest. There is a very high chance that any black resident she stops will fit this statistic. The second officer, Stanley, has the same evidence but does not act on the information because he acknowledges that “the justice system is corrupt and that his other police officers have been using warrants and fines on residents as a way of padding the municipal budget” [@basu2019wrongs]. While he believes the statistic, he does not stop any black resident based solely on the fact that they fit the description in the statistic. In this case, the police department used statistical evidence to allocate officers to an area with a high population of people who have a warrant out for their arrest [@bendetti2019].

Because these technologies are advancing so quickly and are making officer’s jobs more efficient, police departments have jumped at the opportunity to utilize new technologies that make specific predictions about location. Police departments are using statistics to predict the best action for resource allocation in their cities. However, we realize that with this excitement, some consequences are overlooked. The ethics behind police resource allocation to crime hotspots is becoming more prevalent with the implementation of new predictive policing software. 

PredPol can be used positively to decrease department costs, deter criminals, and increase response time. However, predictive policing can also come at a cost to the community. While the dataset that we are using does not include data on race or other identifying features of the accused, when we make predictions based on any statistical evidence it is important to consider its effects on people. It is important to acknowledge that any historical crime data used to make future predictions, no matter the number of data points or years we include, will be biased. Predictive algorithms are "designed to learn and reproduce patterns in data" [@lum2016predict]. However, if we are using already biased data to train these models, our output will create feedback loops which "reproduce and in some cases amplify those same biases" [@lum2016predict]. These feedback loops send officers back to the same area, to collect more data that is based on historically biased data. This data is then fed to these predictive algorithms. Feedback loops are indirectly caused by racial profiling and especially target communities of low income and minority communities. 

The goal of our capstone is to answer the question, "Can we predict future crime in Chicago by analyzing historical crime data?" Police departments are eager to implement new technologies, like PredPol, so we wanted to explore the results of crime forecasting in a large city, like Chicago, that is known for its high crime rates and police department corruption.


## Methods

Our capstone analyzes crime data in Chicago, IL. The dataset that we use for this project is provided by the City of Chicago Data Portal. The rows in the dataset represent individual reported crimes in Chicago from the years 2001 to 2019. The City of Chicago compiled this dataset through the Chicago Police Department's Citizen Law Enforcement Analysis and Reporting (CLEAR) system. The City of Chicago took measures to provide some privacy by releasing block locations rather than the exact location. Due to time restraints, we have decided to focus only on the crimes that occurred within 2018. We did not use the year 2019 because we recognized that we would not have a complete year of data [@CityofChicago].

First, we used areal data to create neighborhood structures of Chicago. We exported a shapefile from the Chicago Data portal that split Chicago into 77 different community areas [@CityofChicago]. Finally, we combined individual community areas into larger regions of Chicago, which we named Region. These regions include the Southside, Central, and the Westside. 

For each reported crime we were given the exact date and time and we separated this variable into the time of day and day of the week. We wrangled the data to create an ideal dataset with the goal to predict crime density by day of the week, time of day, and type of crime within the community area with indicators for region. We computed the crime density by dividing the number of crimes in a given community by the community area’s size. We defined the day of the week by Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, and Sunday. We defined time of day my morning(4am-11pm), afternoon(12pm - 8pm), and night(9pm-3am). We were also given the type of crime, such as battery, homicide, kidnapping, obscenity, public indecency, interference with a public officer, or gambling. We created a variable called Crime.Type where we defined violent and non-violent crimes based on the type of crime. We acknowledge that these categories are a generalization. For example, interference with a public officer can sometimes be violent and sometimes non-violent. However, for the purpose of this project, we classified each crime as violent or non-violent by the frequency of non-violent or violent descriptions for that type.

We modeled crime density using repeated measures data and visualized our crime density using spatial data. Our repeated measures in a community area are defined by different subsets. An example of a subset that we created was the crime density of a region by violent crimes in the morning.

```{r, include=FALSE}
crimelong <- read.csv(file="Crimes2018.csv", header=TRUE, sep=",")
```

This plot shows each individual crime in the dataset. We can learn from this graph that there are enough crimes in our dataset to form the shape of Chicago.  
\
\
```{r, warning=FALSE, results = "hide",fig.align='center'}
crimeppp <- ppp(crimelong$X.Coordinate,crimelong$Y.Coordinate,c(1092706, 1205114),c(1814333, 1951535)) 
plot(crimeppp,pch='.', main= "")
```

This plot was created using `density.ppp()`. We computed and plotted the kernel density estimate of the spatially-varying intensity function. We used a Gaussian kernel with a standard deviation of 2000 units because of the large scale of our data. By using a larger sigma, the plot accounts for a larger spread of crimes. We can see that there is a high density of crimes in the Central region of Chicago. We will explore this later in the report. 

```{r, warning=FALSE, results = "hide", fig.align='center'}
plot(density.ppp(crimeppp, sigma = 2000),  main = "", ribbon=FALSE) 
```

```{r, include=FALSE, warning=FALSE}
#Community Area 
gridCA <- st_read("./CommunityAreas", "geo_export_2611fe40-3e6f-4259-aecc-a51af17d8945")
gridCA$area_num_1 <- as.numeric(as.character(gridCA$area_num_1 ))

Dat_sf_CA <- left_join(gridCA,crimelong, by=c('area_num_1' = 'Community.Area'))

Dat_sf_CA <- st_transform(Dat_sf_CA,"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs") 

summary(Dat_sf_CA)
```


```{r, include=FALSE}
crime_date <- crimelong %>%
  mutate(DateTime = mdy_hm(as.character(Date))) %>%
  mutate(Date = date(DateTime), Hour = hour(DateTime), Month = month(DateTime), Year = year(DateTime))

weekday = c("Mon","Tue","Wed","Thu","Fri","Sat", "Sun")

SUB1 <- crime_date %>%
  select(Date, Primary.Type, Community.Area, Latitude, Longitude, Location, Location.Description, X.Coordinate, Y.Coordinate, Year, DateTime, Hour, Month) %>%
  mutate(DayYear = yday(Date)) %>%
  mutate(DayMonth = mday(Date)) %>%
  mutate(Weekday = wday(Date, label = TRUE)) %>%
  mutate(ToD = ifelse(Hour < 12 & Hour >= 4, "morning", 
                      ifelse(Hour < 18 & Hour >= 12,"afternoon", 
                             ifelse(Hour < 4, 'night',
                                    ifelse(Hour >= 18,'night','NA'))))) %>%
  mutate(DateDec = decimal_date(DateTime))
SUB1 <- SUB1 %>%
    mutate(Friday = ifelse(Weekday =='Fri','Fri',
                           ifelse(Weekday == 'Sun','Sun','NA')))


A <- SUB1 %>%
  count(Community.Area) %>%
  mutate(ArrestCount = n) %>%
  dplyr::select(Community.Area, ArrestCount)
```

```{r, include=FALSE}
SUB1 <- SUB1 %>%
   mutate(violent = ifelse(Primary.Type %in% c('ARSON','ASSAULT', 'CRIM SEXUAL ASSAULT', 'OFFENSE INVOLVING CHILDREN', 'BATTERY', 'HOMICIDE','KIDNAPPING', 'ROBBERY', 'HUMAN TRAFFICKING', 'SEX OFFENSE' ),1,0)) %>%
   mutate(nonviolent = ifelse(Primary.Type %in% c('CONCEALED CARRY LICENSE VIOLATION','DECEPTIVE PRACTICE', 'INTERFERENCE WITH PUBLIC OFFICER', 'MOTOR VEHICLE THEFT', 'OBSCENITY', 'PUBLIC INDECENCY', 'STALKING','GAMBLING', 'INTIMIDATION', 'NARCOTICS', 'PUBLIC PEACE VIOLATION', 'THEFT', 'CRIMINAL DAMAGE', 'NON-CRIMINAL', 'OTHER OFFENSE', 'BURGLARY', 'WEAPONS VIOLATION', 'CRIMINAL TRESPASS', 'LIQUOR LAW VIOLATION', 'NON-CRIMINAL (SUBJECT SPECIFIED)', 'PROSTITUTION'),1,0))  

SUB1 <- SUB1 %>%
  mutate(Crime.Type = factor(ifelse(violent == 1, 'violent','nonviolent')))

```

```{r, include=FALSE}
#separates into 'Regions'
SUB1 <- SUB1 %>%
  mutate(Far_North_Side = ifelse(Community.Area %in% c('1','2','3','4','9','10','12','13','14','76','77'),1,0)) %>%
  mutate(Northwest_Side = ifelse(Community.Area %in% c('15','16','17','18','19','20'),1,0)) %>%
  mutate(North_Side = ifelse(Community.Area %in% c('5', '6', '7','21', '22'),1,0)) %>%
  mutate(West_Side = ifelse(Community.Area %in% c('23','24','25','26','27','28','29','30','31'),1,0)) %>%
  mutate(Central = ifelse(Community.Area %in% c('8','32','33'),1,0)) %>%
  mutate(South_Side = ifelse(Community.Area %in% c('34','35','36','37','38','39','40','41','42','43','60','69'),1,0)) %>%
  mutate(Southwest_Side = ifelse(Community.Area %in% c('56', '57', '58', '59', '61', '62', '63','64','65','66','67','68'),1,0)) %>%
  mutate(Far_Southwest_Side = ifelse(Community.Area %in% c('70','71','72','73','74','75'),1,0)) %>%
  mutate(Far_Southeast_Side = ifelse(Community.Area %in% c('44','45','46','47','48','49','50','51','52','53','54','55'),1,0))

#new column called Region that specifies which side the crimes is in for example "Far_North_Side", "Northwest_Side", "Central", etc.
SUB1$Region = NA #initiates
SUB1$Region[which(SUB1$Far_North_Side == 1)] = 'Far North'
SUB1$Region[which(SUB1$Northwest_Side == 1)] = 'Northwest'
SUB1$Region[which(SUB1$North_Side == 1)] = 'North'
SUB1$Region[which(SUB1$West_Side == 1)] = 'West'
SUB1$Region[which(SUB1$Central == 1)] = 'Central'
SUB1$Region[which(SUB1$South_Side == 1)] = 'South'
SUB1$Region[which(SUB1$Southwest_Side == 1)] = 'Southwest'
SUB1$Region[which(SUB1$Far_Southwest_Side == 1)] = 'Far Southwest'
SUB1$Region[which(SUB1$Far_Southeast_Side == 1)] = 'Far Southeast'

SUB1$Region = factor(SUB1$Region)
```

```{r, include=FALSE}
ModData <- SUB1 %>%
count(Weekday,ToD,Crime.Type,Region,Community.Area, Central)

gridCA$ca_num <- as.numeric(as.character(gridCA$area_num_1))

ModDat_sf_CA <- left_join(gridCA,ModData, by=c('area_num_1' = 'Community.Area'))
ModDat_sf_CA <-st_transform(ModDat_sf_CA,"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs")

ModDat_sf_CA <- ModDat_sf_CA %>%
  mutate(Crime.Density = n/shape_area) %>%
  mutate(Weekday = factor(Weekday,ordered=FALSE))

GEEdat_sf = ModDat_sf_CA %>% filter(!is.na(Region))
GEEdat = GEEdat_sf %>% select(-geometry) %>% as.data.frame() 

```



In the plot below, we exam the number of crimes that occur depending on weekday, time of day and crime type. Our intial findings from this plot are that there is very little variability among the day of the week. Therefore, we are not going to incorporate this variable into our model. We do notice that there is variability in the number of crimes depending on time of day and the type of crime. 
\
\
```{r, fig.align='center'}
crimedenWDTOD <- ggplot() + 
  geom_bar(aes(Weekday, fill=Weekday),data=SUB1) + facet_wrap(~ToD~ Crime.Type) +
  labs(title="Number of Crimes by Weekday, Crime Type and Time of Day", y='Crime Count') +
  theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(angle = 90),legend.position = "none")
# no theme minimal b/c it messes up the vertical labels

crimedenWDTOD
```


Above we examined the effect of weekday on crime density but found that it was not a significant explanatory variable. Next, we explored the relationship of the region and crime type with crime density as shown in the boxplots below. These boxplots show the crime density for each region of Chicago by violent and non-violent crimes. The y-axis represents the crime density and the x-axis shows the different regions of Chicago represented by our Region variable. As we can see, on average, there is a greater density of crime in the Central and West regions of Chicago. There is also a higher overall crime density for non-violent crimes for every region of Chicago.
\
\

```{r,fig.align='center'}
crimedenReg <- ggplot(ModDat_sf_CA, aes(x=Region, y=log(Crime.Density), fill=factor(Region))) + 
  geom_boxplot() + labs(title="Crime Density by Crime Type and Region", y='Log Crime Density') + 
  facet_wrap( ~ Crime.Type) +theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(angle = 90),legend.position = "none")
# no theme minimal b/c it messes up the vertical labels

crimedenReg
```


We created a marginal model using Generalized Estimating Equations (GEE). We chose to use a marginal model because of the advantages that "it involves fewer assumptions" and we are not penalized for using the wrong correlation structure because of the sandwich robust estimator [@hubbard2010gee]. Our data set is very large and marginal models are valid as long as the sample size increases towards infinity [@hubbard2010gee]. We decided not to use a SAR/CAR model because a SAR model would not do well with our dataset which has multiple rows corresponding to each community area. 

Our model predicts the crime density by Region, time of day (categorical) and the type of crime. We used a GEE model with an ar1 correlation structure [@liang1986longitudinal]. This decision was made based on the fact that our data includes more than one row per region so by using a GEE model we have access to robust standard errors. Robust standard errors are crucial for model evaluation because they are valid no matter if we chose the wrong correlation structure. This means that robust standard errors are not sensitive to the specifics of correlation. Within this data, there are two different kinds of correlation. The repeated measures within community areas are correlated and nearby community areas are spatially correlated. 

The slopes are different within each community area because there are different effects from variables like time of day or type of crime. If we do not add an interaction term, we are making the assumption that the crime density is the same throughout regions for the time of day or the type of crime. However, this is not the case. We built the model by initially using an interaction between time of day and weekday but through examining the coefficients we recognized that weekday did not significantly add to the model. Additionally, from the second boxplot, we realized that crime type and time of day could be very significant explanatory variables so we added an interaction with crime type and time of day and another interaction between crime type and region. Using an interaction with these explanatory variables allows us to take into account the region, modifying the effect of crime type and time of day on crime density. This is because interaction considers the difference in slopes of the interacting variables. 

```{r, include=FALSE}
# create geem model
mod <- geem(Crime.Density ~ factor(Crime.Type)*factor(ToD) + factor(Crime.Type)*factor(Region), data = GEEdat, id = area_num_1, corstr = "ar1")
summary(mod)

# add predicitons and residuals to dataset
GEEdat_sf = GEEdat_sf %>% mutate(resid= GEEdat$Crime.Density - predict(mod)) %>% mutate(pred = predict(mod)) %>% mutate(ToD = factor(ToD,levels = c('morning','afternoon','night'))) %>% mutate(logResid = log(resid))
```



## Results

When examining our model summary it is clear that all of the coefficients have small p-values, indicating that the explanatory variables are helpful additions to the model. The slope coefficients reflect the differences between categories. A p-value is the probability of getting an estimated difference between categories. If we have a high p-value, it demonstrates that there is not really a difference between categories, however our model output included small p-values. We use the robust standard errors (Robust SEs) and the model standard errors (Model SEs) to evaluate our correlation structure and find that using an ar1 correlation structure reduces the difference between Robust SEs and model SEs which is important because the closer our Model SEs are to the Robust SEs the better the model. The final aspect we examine for our model is the Wald statistics of our coefficients which measure the estimates divided by the Robust SEs. The Wald statistics for each coefficient are roughly 2.5 to 3.0 standard errors from zero. Coefficients that are more than two standards errors from the estimate inform us that they are far enough from zero that there is no natural random variability. This indicates a significant relationship in the model.

```{r, include = FALSE}
#separates by community area
A <- SUB1 %>%
  count(Community.Area) %>%
  mutate(Crime.Count = n) %>%
  dplyr::select(Community.Area, Crime.Count)

#time - morning and afternoon
#proportion of crime that happen in the morning
B <- SUB1 %>%
  count(Community.Area, ToD) %>%
  group_by(Community.Area) %>%
  mutate(MorningProp = n/sum(n)) %>%
  dplyr::filter(ToD == 'morning') %>%
  dplyr::select(Community.Area, MorningProp) 

C <- SUB1 %>%
  count(Community.Area, Crime.Type) %>%
  group_by(Community.Area) %>%
  mutate(ViolentProp = n/sum(n)) %>%
  dplyr::filter(Crime.Type == 'violent') %>%
  dplyr::select(Community.Area, ViolentProp) 

D <- SUB1 %>%
  count(Community.Area, ToD) %>%
  group_by(Community.Area) %>%
  mutate(AfterNoonProp = n/sum(n)) %>%
  dplyr::filter(ToD == 'afternoon') %>%
  dplyr::select(Community.Area, AfterNoonProp) 

E <- SUB1 %>%
  count(Community.Area, Crime.Type) %>%
  group_by(Community.Area) %>%
  mutate(NonViolentProp = n/sum(n)) %>%
  dplyr::filter(Crime.Type == 'nonviolent') %>%
  dplyr::select(Community.Area, NonViolentProp) 



dat1 <- SUB1  %>%
  select(Region,Community.Area) %>%
  left_join(A) %>%
  left_join(B) %>%
  left_join(C) %>%
  left_join(D) %>%
  left_join(E) %>%
  distinct() 

dat2 <- dat1 %>%
  select()


gridCA$ca_num <- as.numeric(as.character(gridCA$area_num_1))

Dat_sf_CA <- left_join(gridCA,dat1, by=c('area_num_1' = 'Community.Area'))
Dat_sf_CA <-st_transform(Dat_sf_CA,"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=km +no_defs") 

Dat_sf_CA <- Dat_sf_CA %>%
  mutate(Crime.Density = Crime.Count/shape_area)

```



The four plots below illustrate background information about the Chicago land area. We used spatial data to visualize the number of crimes and the crime density in the community areas of Chicago. 

This first visualization shows the number of crimes in Chicago in 2018. The lighter the color of the area, the more crimes. The second visualization takes into account the size of the community area. Here we can see that most of the crime is centralized on the Central side of Chicago. While the community areas are fairly small on the Central side, they have a very high proportion of crimes. 
\
```{r, warning=FALSE,fig.width = 4.5}
ggplot(Dat_sf_CA) + 
  labs( title= "Crime Count by CA") + 
  geom_sf(aes(fill = Crime.Count)) +theme_minimal() 

ggplot(Dat_sf_CA) + 
  labs( title= "Crime Density by CA") + 
  geom_sf(aes(fill = Crime.Density)) +theme_minimal()
```

From the visualization on the left, we can see that the proportion of violent crimes that occur on the Central side is quite low compared to other areas. We can also see that there are not many violent crimes that happen in O'Hare, which is the community area that sticks out from the rest on the upper left-hand side. O’Hare is the location of Chicago airport. We attribute this to the fact that these two areas are very populated. Therefore, we assert that the crimes that are most likely to occur in highly populated are non-violent crimes, like theft. The proportion of non-violent crime plot below further indicates this with more eastern regions of Chicago having high proportions of non-violent crime. We believe that in addition to a higher population in these regions with low violent crime proportion there may also be a higher police presence as well. 
\
```{r, warning=FALSE, fig.width= 4}
ggplot(Dat_sf_CA) + 
  labs( title= "Proportion of Violent Crime by CA") + 
  geom_sf(aes(fill = ViolentProp))+theme_minimal()

ggplot(Dat_sf_CA) + 
  labs( title= "Proportion of Non-Violent Crime by CA") + 
  geom_sf(aes(fill = NonViolentProp))+theme_minimal()
```


The plot below the community areas that are within the Central region. The uppermost community of Central is called Near North, the middle community is the Loop and the lower community is called Near South. 
\
```{r, fig.width=5, fig.align='center'}

GEEdat_sf %>%
  ggplot() +
  labs( title= "Community Areas in the Central Region") + 
  geom_sf(aes(fill = Central)) + 
  theme_minimal() +
  theme(legend.position = "none",plot.title = element_text(hjust = 0.5))

```

The plots below represent the predictions from our model and the residuals for those predictions. 

The predictions indicate high non-violent crime density in the central region of Chicago (especially) and in the surrounding North, West and South regions. The violent crime predictions have less contrast among regions but there is still a slightly higher concentration in the Central and West regions. These predictions support our assumptions because there are, on average, more non-violent crimes than there are violent crimes. Additionally, the highest crime density region is Central and its surrounding regions. 

```{r, results= "hide"}
GEEdat_sf %>%
  ggplot() +
  labs( title= "Model Predictions by Crime Type") + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  geom_sf(aes(fill = pred)) + 
  facet_wrap(~Crime.Type) +
  theme_minimal()
```

Overall, the residual plot indicates good residuals for all the communities across the Chicago land area. It is interesting to notice that some of the Central region communities have slightly higher residuals than some of the other regions. This is concerning because the Central side, in particular, has the greatest crime density for non-violent crime and yet the model did not perform well in this region where it has more data points. We believe this is due to our model being used with all of the Chicago regions. The Central side is an outlier when it comes to crime density compared to other regions and the model struggles with outliers. However, this residual plot is significantly better than our previous one when our model did not consider the region. By adding region our model is able to better handle outlier communities. 
 
```{r, results= "hide",fig.align='center'}
GEEdat_sf %>%
  #filter(Weekday == ) %>%
  ggplot() +
  labs( title= "Model Residuals by Crime Type") + 
  geom_sf(aes(fill = resid)) + 
  scale_fill_gradient(low = ("cornflowerblue"),
  high = ("red"))+
  facet_wrap(~Crime.Type) +
  theme_minimal()
```

From the results above, we decided to focus on the Central Side of Chicago because of the variability we examined above with the residuals and predictions. We believe that if our model was built to forecast crime density in just the high crime density regions of Chicago (Central, North, West, and South) then it would perform better on the Central region by having lower residuals because the high crime density of Central would not be considered an outlier compared to the average. Additionally, we wanted to examine how our model performed with time of day in relation to type of crime. When taking this into account it is clear that the model has lower residuals for violent crime regardless of time of day. For non-violent crime the model has greater residuals in the afternoon and night. We believe this is due to there being less violent crime in the Central region. With fewer observations of violent crime, our model is able to more easily predict crime density because there is a smaller chance of variability. We believe that this is also the case for non-violent crime in the morning. 
\ 
```{r, results= "hide",fig.align='center'}
GEEdat_sf %>%
  filter(Region == 'Central') %>%
  ggplot() +
  labs( title= "Model Residuals by Crime Type and Time of Day") + 
  geom_sf(aes(fill = resid)) + 
  scale_fill_gradient(low = ("cornflowerblue"),
  high = ("red"))+
  facet_wrap(~Crime.Type~ToD) +
  #theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(angle = 90))
```


## Conclusions

Before we created our model, we assumed that we would be able to visualize high-density areas of crime during certain times of the day. However, we found that we were not able to make significant predictions besides the type of crime by region. Aggregating our dataset by community area minimized the dimensions of our dataset. This condensed our dataset to 3,171 observations to account for summaries rather than individual crimes. The smaller dataset is the one that we used to build our model. As a result, the variability in variables like time of day decreased. Therefore, there is less variability in these values and they lose their significance.

While this project does not replicate PrePol predictions, we are attempting to accomplish the same thing on a more general scale. From our model and exploration of the data, we found that most crimes were centered in Central, South, West and North regions and that non-violent crime was concentrated in the Central region. If we were to allocate police resources based on these findings we could allocate more resources to these locations to prevent future crimes. However, if police were to use these predictions to prevent violent crime the following limitations should be addressed: 

One limitation of our model is that it is only trained with one year of data. If we were to include more years in our analysis, we would be able "to reduce the variance and significantly improve the accuracy" of the point process and density visualizations [@mohler2014marked]. Additionally, due to the skew of higher crime density in a small number of certain regions, we recognize that our model does not handle crime forecasts in these regions effectively due to the average crime density being significantly lower in these regions because we are considering the entire Chicago land area. PredPol makes predictions based on "500x500 square foot" [@Lam2019] Therefore, our next steps would be to create a model for each region and then each community area to make more specific predictions. To accomplish this, we would need to use much more data.

Another limitation is our classification of non-violent vs. violent crimes. These classifications were made using broad generalizations. For example, we classified "interference with a public officer" as a non-violent crime, however, we understand that this can also be a violent crime. For the scope of this project, these generalizations were unavoidable. A future direction would include a more reliable method for grouping crimes.

It is also important to acknowledge that any historical crime data used to make future predictions, no matter the number of data points or years we include, will be biased. This dataset included all reported crimes. Therefore, viewers like us, are unaware of the outcome of the arrest. We do not have information about whether the accused was convicted or not. This can lead to heavily biased data and predictions. Regardless of the number of data points, our predictions will be biased because of racial profiling and other factors. Predictive algorithms are "designed to learn and reproduce patterns in data" [@lum2016predict]. However, if we are using already biased data to train these models, our output will create feedback loops which "reproduce and in some cases amplify those same biases" [@lum2016predict]. 

## Acknowledgements

Thank you to our professor, Brianna Heggeseth, for her leadership and guidance in class and in office hours. Thank you to the City of Chicago for their work in compiling this data and making it usable for our project.

## References